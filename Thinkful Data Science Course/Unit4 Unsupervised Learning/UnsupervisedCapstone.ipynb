{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing which authors to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing 10 authors\n",
    "austen = gutenberg.raw('austen-persuasion.txt')\n",
    "bryant = gutenberg.raw('bryant-stories.txt')\n",
    "blake = gutenberg.raw('blake-poems.txt')\n",
    "burgs = gutenberg.raw('burgess-busterbrown.txt')\n",
    "chests = gutenberg.raw('chesterton-ball.txt')\n",
    "carroll = gutenberg.raw('carroll-alice.txt')\n",
    "edges = gutenberg.raw('edgeworth-parents.txt')\n",
    "shakes = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "milton = gutenberg.raw('milton-paradise.txt')\n",
    "whitman = gutenberg.raw('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    text = re.sub(r'CHAPTER \\d+', '', text)\n",
    "    text = re.sub(\"\\\\n\\\\n.*?\\\\n\\\\n\", '', text)\n",
    "  \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all documents\n",
    "austen = text_cleaner(austen)\n",
    "bryant = text_cleaner(bryant)\n",
    "blake = text_cleaner(blake)\n",
    "burgs = text_cleaner(burgs)\n",
    "chests = text_cleaner(chests)\n",
    "carroll = text_cleaner(carroll)\n",
    "edges = text_cleaner(edges)\n",
    "shakes = text_cleaner(shakes)\n",
    "milton = text_cleaner(milton)\n",
    "whitman = text_cleaner(whitman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spacy and analyze the documents\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "austen_doc = nlp(austen)\n",
    "bryant_doc = nlp(bryant)\n",
    "blake_doc = nlp(blake)\n",
    "burgs_doc = nlp(burgs)\n",
    "chests_doc = nlp(chests)\n",
    "carroll_doc = nlp(carroll)\n",
    "edges_doc = nlp(edges)\n",
    "shakes_doc = nlp(shakes)\n",
    "milton_doc = nlp(milton)\n",
    "whitman_doc = nlp(whitman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Sir, Walter, Elliot, ,, of, Kellynch, Hall, ,...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(This, was, the, page, at, which, the, favouri...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(\")</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Precisely, such, had, the, paragraph, origina...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Then, followed, the, history, and, rise, of, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (Sir, Walter, Elliot, ,, of, Kellynch, Hall, ,...  Austen\n",
       "1  (This, was, the, page, at, which, the, favouri...  Austen\n",
       "2                                                (\")  Austen\n",
       "3  (Precisely, such, had, the, paragraph, origina...  Austen\n",
       "4  (Then, followed, the, history, and, rise, of, ...  Austen"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "austen_sents = [[sent, \"Austen\"] for sent in austen_doc.sents]\n",
    "bryant_sents = [[sent, \"Bryant\"] for sent in bryant_doc.sents]\n",
    "blake_sents = [[sent, \"Blake\"] for sent in blake_doc.sents]\n",
    "burgs_sents = [[sent, \"Burgs\"] for sent in burgs_doc.sents]\n",
    "chests_sents = [[sent, \"Chests\"] for sent in chests_doc.sents]\n",
    "carroll_sents = [[sent, \"Carroll\"] for sent in carroll_doc.sents]\n",
    "edges_sents = [[sent, \"Edges\"] for sent in edges_doc.sents]\n",
    "shakes_sents = [[sent, \"Shakes\"] for sent in shakes_doc.sents]\n",
    "milton_sents = [[sent, \"Milton\"] for sent in milton_doc.sents]\n",
    "whitman_sents = [[sent, \"Whitman\"] for sent in whitman_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(austen_sents + bryant_sents + blake_sents + burgs_sents + \n",
    "                         chests_sents + carroll_sents + edges_sents + shakes_sents + milton_sents +\n",
    "                        whitman_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40779"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8414"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "austenwords = bag_of_words(austen_doc)\n",
    "bryantwords = bag_of_words(bryant_doc)\n",
    "blakewords = bag_of_words(blake_doc)\n",
    "burgswords = bag_of_words(burgs_doc)\n",
    "chestswords = bag_of_words(chests_doc)\n",
    "carrollwords = bag_of_words(carroll_doc)\n",
    "edgeswords = bag_of_words(edges_doc)\n",
    "shakeswords = bag_of_words(shakes_doc)\n",
    "miltonwords = bag_of_words(milton_doc)\n",
    "whitmanwords = bag_of_words(whitman_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(austenwords + bryantwords + blakewords + burgswords + \n",
    "                   chestswords + carrollwords + edgeswords + shakeswords + miltonwords + whitmanwords)\n",
    "\n",
    "# How many words we got?\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n"
     ]
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
